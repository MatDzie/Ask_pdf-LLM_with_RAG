{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017-Attention-is-All-You-Need.pdf', '2022-STaR-Self-Taught-Reasoner.pdf', '2023-GPT-4-Technical-Report.pdf', '2023-Lets-Verify-Step-by-Step.pdf', '2023-Sparks-of-AGI.pdf', '2023-Tree-of-Thoughts.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory_path = 'data/'\n",
    "document_list = os.listdir(directory_path)\n",
    "print(document_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse documents into lists of text and create its metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pypdf import PdfReader\n",
    "import uuid\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 50,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "for document_name in document_list:\n",
    "    reader = PdfReader(directory_path + document_name)\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        chunks = text_splitter.create_documents([page_text])\n",
    "        for chunk in chunks:\n",
    "            documents.append(chunk.page_content)\n",
    "            metadatas.append({'source': document_name, 'page_number': reader.get_page_number(page)})\n",
    "            ids.append(str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DB connection and embeddings function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and store documents data as embeddings in vector data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name=\"pdf_data\")\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test querry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['7572c2c5-bf5b-4034-988d-f1784250058a',\n",
       "   '1a516c27-4617-4e46-8329-b0300ad4b0a1',\n",
       "   '1aa6df85-5897-4d37-9fa7-dfda0ee9d8c0',\n",
       "   '29cbf84f-6cc8-43c0-a068-2600147174f1',\n",
       "   '42ec8ba8-84fb-4baf-a498-edcabc6278c4',\n",
       "   '3767b271-11f3-4b51-bcbd-9f0aeab94a6d',\n",
       "   '3004e301-e805-40d7-88c5-dec16bc0de36',\n",
       "   'df22b2d7-13d9-437e-a675-6e66fd6b412f',\n",
       "   '017569b2-7f5f-448a-a364-d635be0e534c',\n",
       "   'a38789e9-7f79-471a-99ef-7293ded66c71']],\n",
       " 'distances': [[0.6938372850418091,\n",
       "   0.7801573872566223,\n",
       "   0.8439009189605713,\n",
       "   0.9090184569358826,\n",
       "   0.920831561088562,\n",
       "   0.9480830430984497,\n",
       "   0.9625264406204224,\n",
       "   0.962557315826416,\n",
       "   0.9701042771339417,\n",
       "   0.972396731376648]],\n",
       " 'metadatas': [[{'page_number': 2,\n",
       "    'source': '2017-Attention-is-All-You-Need.pdf'},\n",
       "   {'page_number': 6, 'source': '2017-Attention-is-All-You-Need.pdf'},\n",
       "   {'page_number': 9, 'source': '2017-Attention-is-All-You-Need.pdf'},\n",
       "   {'page_number': 4, 'source': '2017-Attention-is-All-You-Need.pdf'},\n",
       "   {'page_number': 1, 'source': '2017-Attention-is-All-You-Need.pdf'},\n",
       "   {'page_number': 3, 'source': '2017-Attention-is-All-You-Need.pdf'},\n",
       "   {'page_number': 5, 'source': '2017-Attention-is-All-You-Need.pdf'},\n",
       "   {'page_number': 8, 'source': '2017-Attention-is-All-You-Need.pdf'},\n",
       "   {'page_number': 1, 'source': '2017-Attention-is-All-You-Need.pdf'},\n",
       "   {'page_number': 3, 'source': '2017-Attention-is-All-You-Need.pdf'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['masking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum',\n",
       "   'the approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching',\n",
       "   'arXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10',\n",
       "   '3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].',\n",
       "   'the input or output sequences [ 2,16]. In all but a few cases [ 22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in',\n",
       "   'Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute',\n",
       "   'PEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-',\n",
       "   'to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9',\n",
       "   'described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].',\n",
       "   'extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"attention module\"],\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
