{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Attention-is-All-You-Need.pdf', 'GPT-4-Technical-Report.pdf', 'Lets-Verify-Step-by-Step.pdf', 'Sparks-of-AGI.pdf', 'STaR-Self-Taught-Reasoner.pdf', 'Tree-of-Thoughts.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory_path = 'data/'\n",
    "document_list = os.listdir(directory_path)\n",
    "print(document_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse documents into lists of text and create its metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pypdf import PdfReader\n",
    "import uuid\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "for document_name in document_list:\n",
    "    reader = PdfReader(directory_path + document_name)\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        chunks = text_splitter.create_documents([page_text])\n",
    "        for chunk in chunks:\n",
    "            documents.append(chunk.page_content)\n",
    "            metadatas.append({'source': document_name, 'page_number': reader.get_page_number(page)})\n",
    "            ids.append(str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DB connection and embeddings function\n",
    "Here we are using in memory data base and free model from: https://www.sbert.net/docs/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and store documents data as embeddings in vector data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name=\"pdf_data\", embedding_function=sentence_transformer_ef)\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get user querry, find related information in DB and append it to querry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_querry = \"Hi there! I would like to know why self-attention is so important?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to LLM and extract key information from user prompt\n",
    "This approach gives much better results for vector DB search. \n",
    "\n",
    "Extraction of key information can be performed by smaller LLM for cost efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"importance of self-attention\"\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  temperature=0.0,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You will convert user question to string suitable for data base search. Have in mind that we need only essence for search.\"},\n",
    "    {\"role\": \"user\", \"content\": user_querry}\n",
    "  ]\n",
    ")\n",
    "\n",
    "user_querry_sumerized = completion.choices[0].message.content\n",
    "print(user_querry_sumerized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find revelant .pdf files that might consist answer for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[user_querry_sumerized],\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "# Context can be improved by providing adjacent chunks from the same document based on metadata\n",
    "# In this proof of concept approach of simply combining most relevant chunks of information is good enough\n",
    "context = results[\"documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare system message for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic system message that should be improved by taking into account fragmented context from different files.\n",
    "# Furthermore we can use here metadatas and give LLM instruction to backup its answer with references to documents.\n",
    "system_message = \"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge answer user question.\n",
    "If context does not contain answer, tell the user you didin't find answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answer for user question with high quality LLM and provided context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-attention is important because it allows a model to weigh the importance of different words in a sequence when making predictions. This helps the model capture long-range dependencies and understand the relationships between words in the input sequence. Self-attention has been widely used in natural language processing tasks such as machine translation and text generation, and has been shown to improve the performance of these models.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  temperature=0.0,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_querry}\n",
    "  ]\n",
    ")\n",
    "\n",
    "answer = completion.choices[0].message.content\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
